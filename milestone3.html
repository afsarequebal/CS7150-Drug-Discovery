<!doctype html>
<html lang="en">
  <head>
    <title>CS 7150 Drug Discovery</title>
    <meta property="og:title" content="Your Project Name" />
    <meta name="twitter:title" content="Your Project Name" />
    <meta name="description" content="Your project about your cool topic
      described right here." />
    <meta property="og:description" content="Your project about your cool topic
      described right here." />
    <meta name="twitter:description" content="Your project about your cool topic
      described right here." />
    <meta property="og:type" content="website" />
    <meta name="twitter:card" content="summary" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <!-- bootstrap for mobile-friendly layout -->
    <link rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
      integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N"
      crossorigin="anonymous">
    <script
      src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"
      integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
      crossorigin="anonymous"></script>
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
      crossorigin="anonymous"></script>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700"
      rel="stylesheet">
    <link href="style.css" rel="stylesheet">
    <style>
table {
  font-family: arial, sans-serif;
  border-collapse: collapse;
  width: 100%;
}

td, th {
  border: 1px solid #dddddd;
  text-align: left;
  padding: 8px;
}

tr:nth-child(even) {
  background-color: #dddddd;
}
</style>
  </head>
  <body class="nd-docs">
    <div class="nd-pageheader">
      <div class="container">
        <h1 class="lead">
          <nobr class="widenobr">Graph Convolutions and Drug Discovery</nobr>
          <nobr class="widenobr">For CS 7150</nobr>
        </h1>
      </div>
    </div><!-- end nd-pageheader -->

    <div class="container"> <!-- container 123 -->
      <div class="row"> <!-- row abc -->
        <div class="col justify-content-center text-center"> <!-- cell w/ link to previous write-ups -->
          <h4 style="color:white">
            <a href="index.html">Proposal and Milestone</a></h4>
          <h1>
            Final Report
          </h1>
          <h2>
            Can Graph Convolutions Improve Drug Discovery for Alzheimer's
            Disease?
          </h2>
        </div>
      </div> <!-- end row abc -->
      <br>
      <br>
      <div class="row"> <!-- row introduction -->
        <div class="col">
          <h3>Introduction</h3>
          <p>
            The use of neural networks to predict novel treatments of disease is
            widespread, and includes many areas of medicine and disease.
            Alzheimer's Disease is a well known disease that impacts the lives
            of many people. The amyloid peptides play a large role in the pathogenesis of the
            disease,largely related to their dysfunction <a
              href="https://pubmed.ncbi.nlm.nih.gov/24493463/">[1]</a>. Although
            the Disease impacts such a large population,
            little is known about the underlying biological mechanism as a
            whole, and thus treatments are limited.
            <br><br>
            In 2020, Collins and colleagues <a
              href="https://www.sciencedirect.com/science/article/pii/S0092867420301021?via%3Dihub">[2]</a>
            were able to utilize graph convolutions to produce trainable
            molecular features for use in their neural networks.
            These trainable features were then fed into a feed-forward neural
            network to predict the binary outcome; if the compund
            would act as an antibiotic towards E. coli. This new utilization of
            graph neural networks to train molecular features led to
            the discovery of a novel antibiotic, one that was structurally
            unique to other antibiotics. The proposed project for this course
            involves the reproduction and alteration of Collins work with
            antibiotics to predict novel compounds which may be effective in the
            treatment of
            Alzheimer's Disease, via interaction with the amyloid peptide. <br><br>
          </p>
          <hr>
        </div>
        <br><br>
      </div> <!-- end row introduction -->
      <div class="row"> <!-- row background -->
        <div class="col">
          <h3>Background</h3>
        </div>
      </div> <!-- end row background -->

      <br><br>

      <div class="row"> <!-- row background1 - message passing row -->
        <div class="col"> <!-- col text for mpn -->
          <p>We decided to work with graphs becuase molecular structure
            resembles a graph such that the atoms are
            represented by nodes and bonds are represented by edges.
            The features of a atoms like atom type, and bond type are
            represented in a matrix format. The matrices are stored as node embedding. The drug
            discovery process requires finding similarity in the structure of the molecule. This is
            accomplished by message passing functionality. Each node embedding is multiplied by its neighbor
            node embedding. The number
            of neighbors a node can gather knowledge is defined in the filters
            similar to image convolution
            layers and hence the name graph convolution layers. The
            functionality of filter multiplication
            is described in below figure.
            <br><br>
            The matrix in the right represent the laplacian of the adjacency
            matrix. It can be
            used to create polynomials of the form:
            <img src="laplacian2.png", alt="Graph
              Neural Network" width="450"><br>
            Above polynomial is similar to filter's in CNN's.
            We can make the polynomial of degree 1 and see how the nodes
            combines it's
            node embedding with its neighbors as shown below. Increasing the
            degree will
            increase the length of edges covered by a node.
            <img src="laplacian3.png", alt="Graph
              Neural Network" width="450"><br>
            The update of node embedding after each iteration is described
            below.

            As shown in the right figure, h represents the node embedding. It is
            updated k times which is the
            number of hidden layers. At each layers, the laplacian polynomials
            representing the weights of
            the filters is multiplied with the hidden feature representation to
            update the node embedding g.
            A sigmoid functions is then applied to each node embedding to get
            non-linearity.
            The final output from graph neural network is flatten and passed
            through a feed forward network
            for classification.
          </p>
        </div> <!-- end col text for mpn -->

        <div class="col"> <!-- col mpnimage with message pass image -->
          <img src="background_images.png", alt="Graph
            Neural Network" width="600">
          <br>
         <p> &emsp; *Images taken from articles by <a href = "https://distill.pub/2021/gnn-intro/" >Sanchez</a> 
          and <a href="https://distill.pub/2021/understanding-gnns/">Daigavane</a></p> 
        </div> <!-- end col mpnimage -->
        <br><br>
      </div><!-- end row background1 -->

      <br>
      <hr>
      <div> <!-- Div for entire results section-->
        <h3>Method</h3>
        <br>
        <h5>Data Collection</h5>
        <p>
          Compound and training data was collected from the PubChem Online
          Database. PubChem comtains chemical information
          for more than 9 billion compounds and the results of thousands of
          experiments performed in labs across the world. For
          this analysis, we used experimental data from PubChem Bioassay 1285
          <a
            href="https://pubchem.ncbi.nlm.nih.gov/bioassay/1285">[3]</a>. 
            This assay evaluated 193,771 compounds
          in their ability to inhibit levels of Amyloid Precursor Protein (APP).
          Of these 193,771 compounds, we were able to use the
          PubChem compound database to extract relevant chemical information
          (SMILES) for each of the compounds.
          <br><br>
          For the nomination data, chemical information for 10,000 compounds
          were downloaded from the <a href="https://www.ebi.ac.uk/chembl/"> ChEMBL </a> database,
          another well known chemical library. The purpose of the nomination set
          is to nominate new compounds that have yet to be
          tested in a lab for their ability to reduce levels of the protein. For
          this reason the 10,000 compounds chosen were
          confirmed to not exist in the 193,771 compounds included in the
          original data.
          <br><br>
          <h5>Data Preparation</h5>
          <br>
            <h6><b>Morgan Fingerprints</b></h6>
            <p>Simple feed forward network requires input in the format of a
              matrix. So we have used API from
              <a href="https://www.rdkit.org/"> RDKit</a> library. It takes as input a SMILES string and provides an
              object of molecular structure.
              The object is transformed into vector format by method
              rdMolDescriptors.GetMorganFingerprintAsBitVect
              of RDKit library. This output is called morgan fingerprint. </p>
            <img src="smiles_morgan.png",
              alt="Graph
              Neural Network" width="1050">
            <br>
            <br>
            <div class='row'>
            <div class = 'col'>
              <h6><b>Graph Representations</b></h6>
            <p>Contrary to this, graph neural network works with one hot
              encoding matrices of atom type, bond type and features like chirality, electrical charge of molecules.
              An example of a node object in our libraries would like similar to this: <i>Data(x=[25, 79], edge_index=[2, 54], edge_attr=[54, 10], y=[1]</i>.
              In this example, x is the node index, edge index and edge attribute are edge index, and y is activity label (1), 
              indicating the compound inhibits the level of APP. The figure to the right represents molecular object, its finger
              print representation, adjacency matrix and feature matrices.
            </p>
            </div>
            <br>
            <div class = 'col'> 
              <img style="border:1px solid black;" src="adjMatrix.png",
              alt="Graph
              Neural Network" width="475">
            </div>
          </div>
          <br><br>
          <p>
            DeepChem is a Python library/framework that is often used in the
            Deep Learning and Chemistry fields.
            The framework has integrated many deep learning models and
            functionalities from both PyTorch and TensorFlow.
            Following the extraction of standard chemical data from PubChem,
            graph representations of the molecules were
            needed to pass into our Graph Neural Network models. DeepChem
            provides a function, <i>ConvMolFeaturizer()</i>
            which easily converts approriately formatted graph representations
            by passing in SMILES representations of each
            compound into the function.
            <br><br>
            Due to the nature of drug discovery, the large majority of compounds
            evaluated for activity on proteins
            do not show any activity or impact on levels of the protein. This
            leads to a highly imbalanced dataset.
            In order to account for this, prior to training our models we used
            the DeepChem function <i>BalancingTransformer()</i>
            to minimize the impact of class imbalance on classification tasks.
            The <i>BalancingTransformer</i> function
            accomplishes this by balancing the class weights of both classes, to
            ensure similar impact on model training from
            both classes.
            <br><br>
            The data was split into training, validation, and test sets in a
            80:10:10 ratio. Only 0.89% of tested compounds
            showed activity against the Amyloid Precursor Protein, thus we split
            the data using the <i>RandomStratifiedSplitter()</i>
            function within DeepChem. This ensured the training, validation, and
            test set had equally proportional data in terms of
            class balance.
            <br>
            <br>
            <h5>Model Initialization and Training</h5>
            <br>
            The <b>Support Vector Classifier</b> from the scikit-learn Python library was used as the baseline model 
            to compare with our deep learning models. The model was initialized using the following parameters: 
            <br><br>
            <ul>
              <li>RGB kernel</li>
              <li>class_weight: balanced (to accomodate for class imbalance)</li>
              <li>Squared L2 penalty <i>C = 10.0</i></li>
            </ul>
            <p>Aside from the SVC model, we evaluated three different deep learning models using the listed optimization parameters: </p>
          <h6><b>Feed forward Network Model </b></h6>
              <ul>
                <li>Two Feed forward network of size 1024 and 256 </li>
                <li>Each feed forward Network is followed by dropout of 0.2 </li>
                <li>ADAM optimizer</li>
                <li>Learning Rate 0.0001</li>
                <li>100 epochs</li>
                <li>Cross-Entropy Loss Function</li>
              </ul>

            <b>GraphConvModel</b> <br>
            The GraphConvModel provided by DeepChem was used as the baseline
            Graph Neural Network model for our analyses. It is built on top of the 
            tensorflow Python library. From this model
            we tuned the parameters to determine which architecture provided the
            best performance on our validation set. The architecture
            of the final model included: <br><br>
            <ul>
              <li>Four graph convolutional layers</li>
              <li>One feed-forward fully connected layer</li>
              <li>Dropout (p = 0.5)</li>
              <li>Batch Normalization</li>
              <li>ADAM optimizer</li>
              <li>Learning Rate 0.001</li>
              <li>50 epochs</li>
              <li>Cross-Entropy Loss Function</li>
            </ul>

          <h6><b>Custom Graph Neural network </b></h6>
          The custom GNN was tested with multiple different hyperparameters to find the best performing model. 
          Among these tested parameters include: <br><br>
              <ul>
                <li>Five different variations with number of graph neural network
                  layers = 2, 3, 4,5 and 6</li>
                <li>Feed forward fully-conntect layers of size 16 and 32
                  and a sigmoid function </li>
                <li>ADAM optimizer</li>
                <li>Learning Rate 0.0001</li>
                <li>100 epochs</li>
                <li>Cross-Entropy Loss Function</li>
              </ul>

        </div>
        <br><hr>
        <h3>Results</h3>
        <br>
        <div class="row"> <!-- row -->
          <div class="col"> <!-- col  -->
            The three plots below represent the loss and accuracy metrics after training for 100 epochs. 
            The third image demonstrations how the number of graph convolutional layers greatly 
            impact the performance of the model. This is likely due to the 'message-passing' mechanisms of 
            graph neural networks, in that the number of graph convolutional layers determines how many 'hops'
            each node (atom in our case) can take to communicate or send information to other nodes.
          </div> <!-- end col -->
        </div> <!-- end row -->
        <div class="row"> <!-- row -->
          <div class="col"> <!-- col  -->
            <br><br>
            <img style="border:1px solid black;"  src="graphs3.png", alt="Graph Neural Network" width=1000">
            <br><br>
          </div> <!-- end col -->
        </div> <!-- end row -->
        <div class="row"> <!-- row first row of results -->
          <div class="col"> <!-- col text initial results -->
            <p>
              The confusion matrix for each of our tuned models after passing our test data through the models
              is below. It should be noted that SVC was only 
              able to be run on a subset of the data because of the time taken to train the model. Aside from 
              the SVC, each of the models predicted a considerable number of drugs to be active relative to the 
              true number of compounds. The DeepChem Graph Neural Network however appeared to perform the best 
              when considering the pros and cons of the precision and recall metrics. 
            </p>
              <img src="results_gnn.png", alt="Graph Neural Network" width="1000">
              <br>
          </div>
        </div> <!--end of first row results, text and confusion matrix-->
        <br>
        <div class ="row">
          
          <h5>Nominating New Drugs</h5>
          <p>
            As our Graph Neural Network using the DeepChem library provided the best results 
            according to our needs, we wanted to use an untested library (nomination set) of compounds to evaluate 
            'new' potential drugs that could be tested in-lab. In the biotech industry, this is the 
            case when you are hoping to develop new drugs. 
            <br><br>
            Out of the 10,000 drugs in our nomination set, after creating graph representations and passing 
            them through our trained DeepChem model, 1,366 compounds were predicted as active against the Amyloid 
            Precursor Protein. Although this number is more than we would have liked, we used our knowledge 
            of drug development and biology to further refine these drug nominations and rank them according to 
            various metrics. We will not go into detail, however there is a guideline in the drug development industry 
            that if a compound violates more than one property out of 5 pre-defined properties, there is a good chance 
            the drug will not work in humans <a
            href="https://www.sciencedirect.com/science/article/pii/B9780128037522000119">[4]</a>. Therefore we filtered out compounds that violated any of these 5 assumptions 
            and subsequently ranked them by another drug-nomination metric (QED) <a
            href="https://pubmed.ncbi.nlm.nih.gov/22270643/">[5]</a>. After this procedure we 
            have come up with three new compounds that we are 'nominating' to be explored as novel drugs in the 
            reduction of Amyloid Precursor Protein with the goal of reducing symptoms of Alzheimer's Disease.
          </p>
          <br>
        </div>
        <div class="row"> <!-- row nomination images-->
          <div class="col"> <!-- col nomination images 1 -->
            <h5>CHEMBL1981307</h5>
            <img style="border:1px solid black;" src="drug_1.png",
              alt="Graph
              Neural Network" height="180">
          </div> <!-- end col nomination images 1 -->

          <div class="col"> <!-- col nomination images 2 -->
            <h5>CHEMBL224682</h5>
            <img style="border:1px solid black;" src="drug_2.png",
              alt="Graph
              Neural Network" height="180">
            <br>
          </div> <!-- end col nomination images 2 -->

          <div class="col"> <!-- col nomination images 3 -->
            <h5>CHEMBL234282</h5>
            <img style="border:1px solid black;" src="drug_3.png",
              alt="Graph
              Neural Network" height="180">
            <br>
          </div> <!-- end col nomination images 3 -->
        </div> <!-- end row nomination images-->
        <br>
        <p>* All chemical images retrieved from PubChem.</p>


  <br>
  <hr>
  <!-- <h4>Next Steps</h4>
        <br>
  <ul>
    <li> Model building : The current model implemented in our
      project is the out-of-the-box model provided by DeepChem. We
      would like touse our current results to build off of this model and 
      create custom layers andparameters.
    </li>
    <li> Experiment and performance comparison(In progress):
      We will use support vector machine as an alternative model. It takes
      Morgan fingerprint as an input and performs classification. We will then do comparison
      of the performance of neural network model and SVM. If time
      permits we will try to build random forest model and compare its
      performance with feed-forward neural network model
    </li>
  </ul> -->


  <br>
  <h5><a
      href='https://github.com/hjombach/Drug_Discovery_Alzheimers_Protein'>
      GitHub Link</a></h5>
    <br>
    <hr>
    <h3>References</h3>
    <br>
    <p>
      <a href="https://github.com/chemprop/chemprop">[1] Bloom GS. Amyloid-β and tau: the trigger and bullet in Alzheimer disease pathogenesis. JAMA Neurol.
        2014 Apr;71(4):505-8. doi: 10.1001/jamaneurol.2013.5847. PMID: 24493463.</a>
        <br>
        <br>

      <a href="https://www.sciencedirect.com/science/article/pii/S0092867420301021?via%3Dihub">[2] Stokes, J.M., Yang, K..., Collins, J. (2020).
      A Deep Learning Approach to Antibiotic Discovery, Cell, 180:4, 688-702.</a>
      <br>
      <br>
      <a href="https://pubchem.ncbi.nlm.nih.gov/bioassay/1285"> [3] National Center for Biotechnology Information (2022). PubChem Bioassay Record for AID 1285, Source: Columbia University Molecular 
      Screening Center. Retrieved December 13, 2022 from https://pubchem.ncbi.nlm.nih.gov/bioassay/1285</a>
      <br>
      <br>
      <a href = "https://www.sciencedirect.com/science/article/pii/B9780128037522000119"> [4] Kenakin, Terry P. (2017). Chapter 11 - Pharmacology in Drug Discovery 
        and Development (Second Edition), Pages 275-299, ISBN: 9780128037522. </a>
        <br>
        <br>
      <a href = "https://pubmed.ncbi.nlm.nih.gov/22270643/">[5] Bickerton GR, Paolini GV, Besnard J, Muresan S, Hopkins AL. Quantifying the chemical beauty of 
        drugs. Nat Chem. 2012 Jan 24;4(2):90-8. doi: 10.1038/nchem.1243. PMID: 22270643; PMCID: PMC3524573.</a>
    </p>
    <h2>Team Members</h2>

    <p>Afsar Equebal and Hendrik Ombach. <br>
      <br>


      <!--col</div>--col-->
    </div><!-- end row cat -->
  </div> <!-- end container 123 -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
              </html>



              <!-- 
        <img style="border:1px solid black;"src="laplacian.png" alt="Graph Neural Network" width="500">
          <br>
        <p>
          Filter 
        </p> -->
              <!-- <img style="border:1px solid black;" src="nodeEmbedding.png" alt="Graph Neural Network" width="500"><br>
        <p> Node embedding update</p>
        <br><br>
        <hr>

        <br><br>
        <hr> -->
              <!-- <div class="row">  row background2 - message passing row 

        <div class="col"> col text for laplacian 
          <p>
           The matrix in the right represent the laplacian of the adjacency matrix. It can be used to create polynomials of the form:
           <img src="laplacian2.png", alt="Graph
            Neural Network" width="300"><br>
            Above polynomial is similar to filter's in CNN's.
            We can make the polynomial of degree 1 and see how the nodes combines it's node embedding with its neighbors as shown below. Increasing the degree will increase the length of edges covered by a node.
            <img src="laplacian3.png", alt="Graph
            Neural Network" width="300"><br>
            The update of node embedding after each iteration is described below.
          </p>
        </div> <!-- end col text for graph laplacian 

        <div class="col"> <!-- col laplacian image 
          <img style="border:1px solid black;" src="laplacian.png", alt="Graph
            Neural Network" width="600">
          <br>
        </div> <!-- end col graph_laplacian image
        <br><br>
      </div>end row background2 -->
              <!-- 
      <div class="row">  row background3 - math row 

        <div class="col"> <!-- col text for math
          <p>
            As shown in the right figure, h represents the node embedding. It is updated k times which is the number of hidden layers. At each layers, the laplacian polynomials representing the weights of the filters is multiplied with the hidden feature representation to update the node embedding g. A sigmoid functions is then applied to each node embedding to get non-linearity.
            The final output from graph neural network is flatten and passed through a feed forward network for classification.
          </p>
        </div> <!-- end col text for math

        <div class="col"> <!-- col math image
          <img style="border:1px solid black;" src="nodeEmbedding.png",
            alt="Graph
            Neural Network" width="600">
          <br>
        </div> <!-- end col math image

      </div><!-- end row background3 -->